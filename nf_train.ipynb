{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import time\n",
    "import random\n",
    "from glob import glob\n",
    "from datetime import datetime\n",
    "\n",
    "import align.detect_face\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from scipy import misc\n",
    "from scipy.interpolate import Rbf\n",
    "from skimage import io, transform\n",
    "from six.moves import xrange\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "\n",
    "import facenet\n",
    "import helper\n",
    "\n",
    "#Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.ion() # interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks_frame = pd.read_csv('./face_landmarks_generate.csv')\n",
    "file_list = landmarks_frame.image_name.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgP_container = np.load('f_avgP_list.npz')\n",
    "emb_container = np.load('f_emb_list.npz')\n",
    "\n",
    "train_set = []\n",
    "test_set = []\n",
    "\n",
    "for key in sorted(emb_container, key=lambda x: int(x.strip('arr_'))) :\n",
    "    batch = avgP_container[key], emb_container[key]\n",
    "    if len(batch[0]) is batch_size :\n",
    "        train_set.append(batch)\n",
    "    else :\n",
    "        test_set.append(batch)\n",
    "\n",
    "test_set_index = len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7867, 160, 160, 3)\n"
     ]
    }
   ],
   "source": [
    "t_dataset = helper.Dataset('nf',file_list, 160)\n",
    "print(t_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "983\n"
     ]
    }
   ],
   "source": [
    "print(len(train_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F_layer(encoded, f_num = 128) : \n",
    "    with tf.variable_scope('F') :\n",
    "        fc = slim.fully_connected(encoded, f_num, activation_fn=tf.nn.relu, scope='fc')\n",
    "    return fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP(net, landmark_num = 68, reuse=None, scope='MLP'):\n",
    "    \"\"\"Builds the MLP for landmark\"\"\"\n",
    "    with tf.variable_scope(scope, 'MLP') :\n",
    "        net = slim.fully_connected(net, 256, activation_fn=None, scope='fc0')\n",
    "        net = slim.fully_connected(net, 128, activation_fn=None, scope='fc1')\n",
    "        net = slim.fully_connected(net, landmark_num, activation_fn=tf.nn.relu, scope='fc2')\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def landmark_decode(net, landmark_num = 68):\n",
    "    with tf.variable_scope('landmark') :\n",
    "        decoded_x = MLP(net, scope= 'decoded_x')\n",
    "        decoded_y = MLP(net, scope= 'decoded_y')\n",
    "    return decoded_x, decoded_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN(F, size) :\n",
    "    with tf.variable_scope('CNN') :\n",
    "        # 12 x 12 x 256\n",
    "        f_size = int(size / 8)\n",
    "        features = slim.fully_connected(F, f_size * f_size * 256, activation_fn=None, scope=\"features\")\n",
    "        features = tf.reshape(features, [-1, f_size, f_size, 256])\n",
    "        # print(features.shape)\n",
    "        \n",
    "        # 24 x 24 x 128\n",
    "        upsample_0 = slim.conv2d_transpose(features, 128, 5, stride=2, scope=\"upsample_0\")\n",
    "        # print(upsample_0.shape)\n",
    "        \n",
    "        # 48 x 48 x 64\n",
    "        upsample_1 = slim.conv2d_transpose(upsample_0, 64, 5, stride=2, scope=\"upsample_1\")\n",
    "        # print(upsample_1.shape)\n",
    "        \n",
    "        # 96 x 96 x 32\n",
    "        upsample_2 = slim.conv2d_transpose(upsample_1, 32, 5, stride=2, scope=\"upsample_2\")\n",
    "        # print(upsample_2.shape)\n",
    "        \n",
    "        # 96 x 96 x 3\n",
    "        one_by_one_conv = slim.conv2d(upsample_2, 3, 1, stride=1, activation_fn=None, scope=\"one_by_one_conv\")\n",
    "        # print(one_by_one_conv.shape)\n",
    "    return one_by_one_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def texture_decode(net, size) :\n",
    "    with tf.variable_scope('texture') :\n",
    "        cnn = CNN(net, size)\n",
    "    return cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grids(size):\n",
    "    return np.mgrid[0:size-1:(size * 1j), 0:size-1:(size * 1j)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_zero_displacement(size):\n",
    "    mid = size/2\n",
    "    end = size-1\n",
    "    \n",
    "    zero_displacement = [[0,0], \n",
    "                         [0, mid], \n",
    "                         [0, end], \n",
    "                         [mid, 0], \n",
    "                         [end,0], \n",
    "                         [end, mid], \n",
    "                         [end, end], \n",
    "                         [mid, end]]\n",
    "    return zero_displacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbf_tf(pred_x, pred_y, correct_points, grids, grid_shape):\n",
    "    def _euclidean_norm_tf(x1, x2):    \n",
    "        return tf.sqrt(tf.reduce_sum(((x1 - x2)**2), 1))\n",
    "\n",
    "    def _h_linear_tf(r):\n",
    "        return r\n",
    "\n",
    "    def _call_norm_tf(x1, x2):\n",
    "        x1 = tf.expand_dims(x1, 3) \n",
    "        x2 = tf.expand_dims(x2, 2) \n",
    "        return norm(x1, x2)    \n",
    "\n",
    "    # set parameters\n",
    "    norm = _euclidean_norm_tf\n",
    "    basis_function = _h_linear_tf\n",
    "    epsilon = tf.constant(2.)\n",
    "    smooth = tf.constant(1.)\n",
    "\n",
    "    #xi = tf.concat([tf.expand_dims(pred_x, 1), tf.expand_dims(pred_y, 1)], 1) # (None, 2, 76)\n",
    "    xi = tf.stack([pred_x, pred_y], axis= 1)\n",
    "    N = xi.shape[-1].value # same as landmarks_num => 76\n",
    "    di = tf.expand_dims(correct_points, 2) # (None, 76, 1)\n",
    "    \n",
    "    r = _call_norm_tf(xi, xi) # (None, 76, 76)\n",
    "    \n",
    "    batch_shape = tf.shape(pred_x)[0:1]\n",
    "    A = tf.subtract(basis_function(r), tf.multiply(smooth, tf.eye(N, batch_shape= batch_shape)))\n",
    "\n",
    "    nodes = tf.matrix_solve (A, di)\n",
    "    r2 = _call_norm_tf(grids, xi)\n",
    "    return tf.reshape(tf.matmul(r2, nodes), [-1, grid_shape[0], grid_shape[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warp_tf(data, pred_x, pred_y, correct_x, correct_y, grids, grid_shape, zero_displacement) :\n",
    "    \n",
    "    pred_x_zd = tf.concat([pred_x, zero_displacement[0]], axis=1)\n",
    "    pred_y_zd = tf.concat([pred_y, zero_displacement[0]], axis=1)\n",
    "    correct_x_zd = tf.concat([correct_x, zero_displacement[1]], axis=1)\n",
    "    correct_y_zd = tf.concat([correct_y, zero_displacement[1]], axis=1)\n",
    "    \n",
    "    rbf_x = rbf_tf(pred_x_zd, pred_y_zd, correct_x_zd, grids, grid_shape)\n",
    "    rbf_y = rbf_tf(pred_x_zd, pred_y_zd, correct_y_zd, grids, grid_shape)\n",
    "    warp = tf.stack([rbf_x, rbf_y], axis= 3)\n",
    "    # warp = tf.concat([tf.expand_dims(rbf_x, 3), tf.expand_dims(rbf_y, 3)], axis=3)\n",
    "    resample = tf.contrib.resampler.resampler(data=data, warp=warp)\n",
    "    return resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logs/20171016-045330\n"
     ]
    }
   ],
   "source": [
    "# summary\n",
    "base_dir = 'logs'\n",
    "subdir = datetime.strftime(datetime.now(), '%Y%m%d-%H%M%S')\n",
    "log_dir = os.path.join(base_dir, subdir)\n",
    "print(log_dir)\n",
    "# log_dir = os.path.join(os.path.expanduser(args.logs_base_dir), subdir)\n",
    "\n",
    "# hyperparam\n",
    "epochs = 200\n",
    "\n",
    "avgP_num = 1792\n",
    "emb_num = 128\n",
    "f_num = 128\n",
    "\n",
    "l_num = 68\n",
    "zd_l_num = 76\n",
    "t_size = 160\n",
    "t_channel = 3\n",
    "\n",
    "grid_y, grid_x = get_grids(t_size)\n",
    "grid_shape = grid_x.shape\n",
    "xa = np.asarray([a.flatten() for a in [grid_x, grid_y]], dtype=np.float_) # (2, 25600)\n",
    "xa = np.asarray([xa for _ in range(0, batch_size)], dtype=np.float_) # (batch_size, 2, 25600)\n",
    "\n",
    "zd = get_zero_displacement(t_size)\n",
    "zd = np.asarray([zd for _ in range(0, batch_size)], dtype=np.float_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    # placeholder\n",
    "    avgP_inputs = tf.placeholder(tf.float32, (None, avgP_num), name='avgP_inputs')\n",
    "    \n",
    "    l_x_labels = tf.placeholder(tf.float32, (None, l_num), name='l_x_labels')\n",
    "    l_y_labels = tf.placeholder(tf.float32, (None, l_num), name='l_y_labels')\n",
    "    t_labels = tf.placeholder(tf.float32, (None, t_size, t_size, t_channel), name='t_labels')\n",
    "    w_labels = tf.placeholder(tf.float32, shape=(None, emb_num), name= 'w_labels')\n",
    "    \n",
    "    grids = tf.constant(xa, dtype=tf.float32, name= 'grids')\n",
    "    zero_displacement = (tf.constant(zd[:, :, 0], dtype=tf.float32, name= 'zd_x'), \n",
    "                         tf.constant(zd[:, :, 1], dtype=tf.float32, name= 'zd_y'))\n",
    "    \n",
    "    # model\n",
    "    F = F_layer(avgP_inputs, f_num= f_num)\n",
    "    \n",
    "    (l_x_preds, l_y_preds) = landmark_decode(F, landmark_num= l_num)\n",
    "    \n",
    "    l_x_loss = tf.losses.mean_squared_error(l_x_labels, l_x_preds, reduction=\"weighted_mean\")\n",
    "    l_y_loss = tf.losses.mean_squared_error(l_y_labels, l_y_preds, reduction=\"weighted_mean\")\n",
    "    \n",
    "    l_loss = tf.add(l_x_loss, l_y_loss)\n",
    "    \n",
    "    t_preds = texture_decode(F, t_size)\n",
    "    t_loss = tf.losses.absolute_difference(t_labels, t_preds)\n",
    "    \n",
    "    warp = warp_tf(t_preds, l_x_preds, l_y_preds, l_x_labels, l_y_labels, grids, grid_shape, zero_displacement)\n",
    "    cast_warp = tf.cast(warp, tf.uint8)\n",
    "    \n",
    "    w_loss = tf.losses.absolute_difference(t_labels, warp)\n",
    "    \n",
    "    total_cost = l_loss + t_loss + w_loss\n",
    "    opt = tf.train.AdamOptimizer(0.001).minimize(total_cost)\n",
    "    \n",
    "    sess = tf.Session()\n",
    "    \n",
    "    for var in tf.trainable_variables():\n",
    "        tf.summary.histogram(var.op.name, var)\n",
    "    \n",
    "    summary_op = tf.summary.merge_all()\n",
    "    summary_writer = tf.summary.FileWriter(log_dir, sess.graph)    \n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with g.as_default():\n",
    "#     time_load_data = time.time()\n",
    "    \n",
    "#     #facenet\n",
    "#     start_load_facenet = time.time()\n",
    "#     print(\"--- %s start load facenet ---\" % (start_load_facenet))\n",
    "#     facenet.load_model('./20171012', input_map={\"input:0\": warp})\n",
    "#     print(\"--- %s facenet loaded ---\" % (time.time() - start_load_facenet))\n",
    "\n",
    "#     f_phase_train_placeholder = tf.get_default_graph().get_tensor_by_name(\"phase_train:0\")\n",
    "#     w_preds = tf.get_default_graph().get_tensor_by_name(\"embeddings:0\")    \n",
    "#     w_loss = tf.losses.cosine_distance(tf.nn.l2_normalize(w_labels, 1), \n",
    "#                                        tf.nn.l2_normalize(w_preds, 1), dim=1)\n",
    "    \n",
    "#     total_cost = l_loss + t_loss + w_loss\n",
    "    \n",
    "#     opt = tf.train.AdamOptimizer(0.001).minimize(total_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_landmarks(image, landmarks):\n",
    "    plt.imshow(image)\n",
    "    plt.scatter(landmarks[:, 0], landmarks[:, 1], s=10, marker='.', c='b')\n",
    "    plt.pause(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_test(l_x, l_y, t, index):\n",
    "    index = len(train_set) * batch_size + index\n",
    "    img_name = landmarks_frame.ix[index, 0]\n",
    "    landmarks = landmarks_frame.ix[index, 1:].as_matrix().astype('float')\n",
    "    landmarks = landmarks.reshape(-1, 2) # (68, 2)\n",
    "    \n",
    "    plt.figure()\n",
    "    show_landmarks(io.imread(os.path.join('', img_name)), landmarks)\n",
    "    plt.show()\n",
    "    \n",
    "    t_img = scipy.misc.toimage(t)\n",
    "    plt.imshow(t_img)\n",
    "    plt.scatter(l_x, l_y, s=10, marker='.', c='b')\n",
    "    plt.pause(0.001)\n",
    "    plt.show()\n",
    "\n",
    "#     plt.imshow(np.squeeze(w, axis=0))\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 1, 1, 1792)\n"
     ]
    }
   ],
   "source": [
    "print(test_set[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 1/983 Training loss: X = 886.6674, Y = 939.4503, T = 156.1129, W = 156.1129\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Nan in summary histogram for: F/fc/weights_1\n\t [[Node: F/fc/weights_1 = HistogramSummary[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](F/fc/weights_1/tag, F/fc/weights/read/_47)]]\n\nCaused by op 'F/fc/weights_1', defined at:\n  File \"/home/carnd/anaconda3/lib/python3.5/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/carnd/anaconda3/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/carnd/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/home/carnd/anaconda3/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/carnd/anaconda3/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/carnd/anaconda3/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/carnd/anaconda3/lib/python3.5/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/carnd/anaconda3/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/carnd/anaconda3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/carnd/anaconda3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/carnd/anaconda3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/carnd/anaconda3/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/carnd/anaconda3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/carnd/anaconda3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/carnd/anaconda3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/carnd/anaconda3/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/carnd/anaconda3/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/carnd/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/carnd/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/carnd/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-17-060f9913592d>\", line 39, in <module>\n    tf.summary.histogram(var.op.name, var)\n  File \"/home/carnd/anaconda3/lib/python3.5/site-packages/tensorflow/python/summary/summary.py\", line 192, in histogram\n    tag=tag, values=values, name=scope)\n  File \"/home/carnd/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/gen_logging_ops.py\", line 129, in _histogram_summary\n    name=name)\n  File \"/home/carnd/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"/home/carnd/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2630, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/carnd/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1204, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Nan in summary histogram for: F/fc/weights_1\n\t [[Node: F/fc/weights_1 = HistogramSummary[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](F/fc/weights_1/tag, F/fc/weights/read/_47)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Nan in summary histogram for: F/fc/weights_1\n\t [[Node: F/fc/weights_1 = HistogramSummary[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](F/fc/weights_1/tag, F/fc/weights/read/_47)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-22b1e1cdd2ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m#                 summary, l_x_cost, l_y_cost, t_cost, _ = sess.run(run, feed_dict= feed_dict)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                 \u001b[0msummary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_x_cost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_y_cost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_cost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_cost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m#                 if i % 10 == 0 :\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1340\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Nan in summary histogram for: F/fc/weights_1\n\t [[Node: F/fc/weights_1 = HistogramSummary[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](F/fc/weights_1/tag, F/fc/weights/read/_47)]]\n\nCaused by op 'F/fc/weights_1', defined at:\n  File \"/home/carnd/anaconda3/lib/python3.5/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/carnd/anaconda3/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/carnd/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/home/carnd/anaconda3/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/carnd/anaconda3/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/carnd/anaconda3/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/carnd/anaconda3/lib/python3.5/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/carnd/anaconda3/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/carnd/anaconda3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/carnd/anaconda3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/carnd/anaconda3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/carnd/anaconda3/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/carnd/anaconda3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/carnd/anaconda3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/carnd/anaconda3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/carnd/anaconda3/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/carnd/anaconda3/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/carnd/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/carnd/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/carnd/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-17-060f9913592d>\", line 39, in <module>\n    tf.summary.histogram(var.op.name, var)\n  File \"/home/carnd/anaconda3/lib/python3.5/site-packages/tensorflow/python/summary/summary.py\", line 192, in histogram\n    tag=tag, values=values, name=scope)\n  File \"/home/carnd/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/gen_logging_ops.py\", line 129, in _histogram_summary\n    name=name)\n  File \"/home/carnd/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"/home/carnd/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2630, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/carnd/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1204, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Nan in summary histogram for: F/fc/weights_1\n\t [[Node: F/fc/weights_1 = HistogramSummary[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](F/fc/weights_1/tag, F/fc/weights/read/_47)]]\n"
     ]
    }
   ],
   "source": [
    "with g.as_default():\n",
    "    with sess.as_default() :\n",
    "        start_test = time.time()\n",
    "        \n",
    "        for e in range(epochs):\n",
    "            for i, ((f_avgP, f_emb), t_label_batch) in enumerate(zip(train_set, t_dataset.get_batches(batch_size))):\n",
    "                start = i * batch_size\n",
    "                end = min(start+batch_size, len(train_set) * batch_size)\n",
    "                size = end - start\n",
    "\n",
    "                l_labels = landmarks_frame.ix[start:end - 1, 1:].as_matrix().astype('float').reshape(size, l_num, 2)\n",
    "                \n",
    "#                 run = [summary_op,\n",
    "#                        l_x_loss, \n",
    "#                        l_y_loss,\n",
    "#                        t_loss,\n",
    "#                        opt]\n",
    "                \n",
    "                run = [summary_op,\n",
    "                       l_x_loss, \n",
    "                       l_y_loss, \n",
    "                       t_loss, \n",
    "                       w_loss,\n",
    "                       opt]\n",
    "                \n",
    "#                 feed_dict = {avgP_inputs : f_avgP.reshape(-1, avgP_num),\n",
    "#                              l_x_labels : l_labels[:, :, 0].reshape(-1, l_num), \n",
    "#                              l_y_labels : l_labels[:, :, 1].reshape(-1, l_num),\n",
    "#                              t_labels : t_label_batch}\n",
    "                \n",
    "                feed_dict = {avgP_inputs : f_avgP.reshape(-1, avgP_num),\n",
    "                             l_x_labels : l_labels[:, :, 0].reshape(-1, l_num), \n",
    "                             l_y_labels : l_labels[:, :, 1].reshape(-1, l_num),\n",
    "                             t_labels : t_label_batch, \n",
    "                             w_labels : f_emb.reshape(-1, emb_num)}\n",
    "                             #w_labels : f_emb.reshape(-1, emb_num), \n",
    "                             #f_phase_train_placeholder:False}\n",
    "\n",
    "#                 summary, l_x_cost, l_y_cost, t_cost, _ = sess.run(run, feed_dict= feed_dict)\n",
    "                summary, l_x_cost, l_y_cost, t_cost, w_cost, _ = sess.run(run, feed_dict= feed_dict)\n",
    "                \n",
    "#                 if i % 10 == 0 :\n",
    "#                     print(\"Iter: {}/{}\".format(i+1, len(train_set)),\\\n",
    "#                               \"Training loss: X = {:.4f}, Y = {:.4f}, T = {:.4f}\".format(l_x_cost / size, \n",
    "#                                                                                          l_y_cost / size, \n",
    "#                                                                                          t_cost))\n",
    "                if i % 10 == 0 :\n",
    "                    print(\"Iter: {}/{}\".format(i+1, len(train_set)),\\\n",
    "                              \"Training loss: X = {:.4f}, Y = {:.4f}, T = {:.4f}, W = {:.4f}\".format(l_x_cost / size, \n",
    "                                                                                                     l_y_cost / size, \n",
    "                                                                                                     t_cost, \n",
    "                                                                                                     w_cost))\n",
    "                summary_writer.add_summary(summary, e * len(train_set) + i)\n",
    "\n",
    "#             test_index = random.randint(0, len(test_set[0])-1)\n",
    "#             test_avgP = test_set[0][0][test_index]\n",
    "#             test_run = [l_x_preds, l_y_preds, t_preds]\n",
    "#             test_feed = {avgP_inputs : test_avgP.reshape(-1, avgP_num)}\n",
    "#             t_l_x, t_l_y, t_t = sess.run(test_run, feed_dict= test_feed)\n",
    "#             show_test(t_l_x.reshape(l_num), \n",
    "#                       t_l_y.reshape(l_num), \n",
    "#                       t_t.reshape(t_size, t_size, t_channel), test_index)\n",
    "            \n",
    "            print(\"Epoch: {}/{}\".format(e+1, epochs), \"Time: %s\" % (time.time() - start_test))\n",
    "            \n",
    "            saver = tf.train.Saver()\n",
    "            save_path = saver.save(sess, \"./tmp/model_1.ckpt\")\n",
    "            print(\"Model saved in file: %s\" % save_path, \"Time: %s\" % (time.time() - start_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeding Facenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_align_data(image_paths, image_size, margin, gpu_memory_fraction):\n",
    "\n",
    "    minsize = 20 # minimum size of face\n",
    "    threshold = [ 0.6, 0.7, 0.7 ]  # three steps's threshold\n",
    "    factor = 0.709 # scale factor\n",
    "    \n",
    "    print('Creating networks and loading parameters')\n",
    "    with tf.Graph().as_default():\n",
    "        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_memory_fraction)\n",
    "        sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\n",
    "        with sess.as_default():\n",
    "            pnet, rnet, onet = align.detect_face.create_mtcnn(sess, None)\n",
    "  \n",
    "    nrof_samples = len(image_paths)\n",
    "    img_list = [None] * nrof_samples\n",
    "    for i in range(nrof_samples):\n",
    "        img = misc.imread(os.path.expanduser(image_paths[i]))\n",
    "        if (img.shape[2] == 4):\n",
    "            img = img[:, :, :3]\n",
    "        img_size = np.asarray(img.shape)[0:2]\n",
    "        bounding_boxes, _ = align.detect_face.detect_face(img, minsize, pnet, rnet, onet, threshold, factor)\n",
    "        det = np.squeeze(bounding_boxes[0,0:4])\n",
    "        bb = np.zeros(4, dtype=np.int32)\n",
    "        bb[0] = np.maximum(det[0]-margin/2, 0)\n",
    "        bb[1] = np.maximum(det[1]-margin/2, 0)\n",
    "        bb[2] = np.minimum(det[2]+margin/2, img_size[1])\n",
    "        bb[3] = np.minimum(det[3]+margin/2, img_size[0])\n",
    "        cropped = img[bb[1]:bb[3],bb[0]:bb[2],:]\n",
    "        aligned = misc.imresize(cropped, (image_size, image_size), interp='bilinear')\n",
    "        prewhitened = facenet.prewhiten(aligned)\n",
    "        img_list[i] = prewhitened\n",
    "        sys.stdout.write('\\r'+ '%d/%d'%(i, nrof_samples))\n",
    "    images = np.stack(img_list)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-6ad8c0b8b690>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mf_emb_g\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mf_emb_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mf_avgP_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "\n",
    "f_emb_g = tf.Graph()\n",
    "f_emb_list = []\n",
    "f_avgP_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'f_emb_g' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3ce0768e5fa8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mf_emb_g\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mtime_load_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"--- data loading start ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_and_align_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m110\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"--- %s data loaded ---\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime_load_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'f_emb_g' is not defined"
     ]
    }
   ],
   "source": [
    "with f_emb_g.as_default():\n",
    "    time_load_data = time.time()\n",
    "    print(\"--- data loading start ---\")\n",
    "    images = load_and_align_data(file_list[:10], 110, 0, 1.0)\n",
    "    print(\"--- %s data loaded ---\" % (time.time() - time_load_data))\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        start_load_facenet = time.time()\n",
    "        print(\"--- %s start load facenet ---\" % (start_load_facenet))\n",
    "        facenet.load_model('./20171012')\n",
    "        print(\"--- %s facenet loaded ---\" % (time.time() - start_load_facenet))\n",
    "\n",
    "        # Get input and output tensors\n",
    "        f_inputs = tf.get_default_graph().get_tensor_by_name(\"input:0\")\n",
    "        f_phase_train_placeholder = tf.get_default_graph().get_tensor_by_name(\"phase_train:0\")\n",
    "        f_avgP_logits = tf.get_default_graph().get_tensor_by_name(\"InceptionResnetV1/Logits/AvgPool_1a_8x8/AvgPool:0\")\n",
    "        f_emb_logits = tf.get_default_graph().get_tensor_by_name(\"embeddings:0\")\n",
    "        \n",
    "        start_test = time.time()\n",
    "        \n",
    "        total_size = len(images)\n",
    "        for i in range(0, total_size, batch_size):\n",
    "            batch = images[i:min(i+batch_size, total_size)]\n",
    "            f_feed = {f_inputs: batch, f_phase_train_placeholder:False}\n",
    "            f_embedings, f_avgPool = sess.run([f_emb_logits, f_avgP_logits], feed_dict=f_feed)\n",
    "            f_emb_list.append(f_embedings)\n",
    "            f_avgP_list.append(f_avgPool)\n",
    "            sys.stdout.write('\\r'+ \"[%d/%d] %s\" % (i, total_size, time.time() - start_test))\n",
    "            print()\n",
    "        print(\"--- %s seconds ---\" % (time.time() - start_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('f_emb_list.npz', *f_emb_list)\n",
    "np.savez('f_avgP_list.npz', *f_avgP_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7867, 160, 160, 3)\n"
     ]
    }
   ],
   "source": [
    "print(images.shape)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
